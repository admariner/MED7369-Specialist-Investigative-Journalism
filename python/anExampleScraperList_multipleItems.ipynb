{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "anExampleScraperList_multipleItems.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAT7xLYe_scU"
      },
      "source": [
        "# An example scraper - this time grabbing more than one piece of info\n",
        "\n",
        "The code below can be copied and adapted to create your own scraper. This builds on [a previous scraper which introduced the use of lists in scraping](https://github.com/paulbradshaw/MED7369-Specialist-Investigative-Journalism/blob/master/python/anExampleScraperList.ipynb).\n",
        "\n",
        "The first part installs all the libraries..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sE2vW-IX9kYX"
      },
      "source": [
        "#install the libraries \n",
        "#scraperwiki is a library for scraping webpages\n",
        "!pip install scraperwiki\n",
        "import scraperwiki\n",
        "#lxml.html is used to convert it into xml (more structured)\n",
        "import lxml.html\n",
        "#cssselect is used to drill down into that and find data in tags\n",
        "!pip install cssselect\n",
        "import cssselect\n",
        "#the pandas library which is used to work with data - we call it 'pd' here so we have to type less!\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn8Zsx4fiC_u"
      },
      "source": [
        "## The previous code\n",
        "\n",
        "Previously we looped through each item in a list and added it to a base url using the `+` operator, then scraped something from that URL.\n",
        "\n",
        "We also stored the results in a dataframe. \n",
        "\n",
        "Here's the code that we got to:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Yw0QKTJ_qpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f45ad63b-a3d6-4366-d662-bab3f5014808"
      },
      "source": [
        "#create a list of counties that we will need to generate URLs\n",
        "counties = [\"avon\",\"bedfordshire\",\"berkshire\",\"birmingham\"]\n",
        "#store the base URL we will add those to\n",
        "baseurl = \"http://www.uk-go-karting.com/tracks/\"\n",
        "\n",
        "#Create a dataframe to store the data we are about to scrape\n",
        "#It has one column called 'title'\n",
        "#We call this dataframe 'df'\n",
        "df = pd.DataFrame(columns=[\"location\"])\n",
        "\n",
        "#start looping through our list\n",
        "for county in counties:\n",
        "  fullurl = baseurl+county\n",
        "  print(fullurl)\n",
        "  #Scrape the html at that url\n",
        "  html = scraperwiki.scrape(fullurl)\n",
        "  # turn our HTML into an lxml object\n",
        "  root = lxml.html.fromstring(html) \n",
        "  #There are 100 recordings on the page\n",
        "  #The titles are all in <div class=\"title\"> and then <a \n",
        "  #This targets the contents of those html tags\n",
        "  addresses = root.cssselect('h3')\n",
        "  #the results are always a list so we have to loop through it using a 'for' loop\n",
        "  for i in addresses:\n",
        "    #each item in the list is called i as it loops\n",
        "    print(i)\n",
        "    #on its own it looks odd, but we can attach .text_content() to translate it into text\n",
        "    address = i.text_content()\n",
        "    print(address)\n",
        "    #Now we need to store it in that variable called 'df' \n",
        "    df = df.append({\n",
        "      \"location\" : address\n",
        "      }, ignore_index=True)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "http://www.uk-go-karting.com/tracks/avon\n",
            "<Element h3 at 0x7fa797350cb0>\n",
            "Avonmouth Way, Bristol, Avon BS11 9YA\n",
            "http://www.uk-go-karting.com/tracks/bedfordshire\n",
            "<Element h3 at 0x7fa79736cbf0>\n",
            "Unit 27, Verey Road, Woodside Industrial Estate, Dunstable, Bedfordshire LU5 4TT\n",
            "http://www.uk-go-karting.com/tracks/berkshire\n",
            "<Element h3 at 0x7fa79736cef0>\n",
            "Cradock Road, Reading, Berkshire RG2 0EE\n",
            "http://www.uk-go-karting.com/tracks/birmingham\n",
            "<Element h3 at 0x7fa79736cd70>\n",
            "Fazeley Street, Birmingham B5 5SE\n",
            "<Element h3 at 0x7fa79737d050>\n",
            "Adderley Road South, Birmingham B8 1AD\n",
            "<Element h3 at 0x7fa79737d170>\n",
            "Park Lane, Oldbury, Birmingham B69 4JX\n",
            "<Element h3 at 0x7fa79737d2f0>\n",
            "Robeys Lane, Tamworth,  B78 1AR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIdecPU__881",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13d13735-c509-4eeb-a90d-c355404704fb"
      },
      "source": [
        "#Once the loop has finished we can take a look at the data\n",
        "print(df)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                            location\n",
            "0              Avonmouth Way, Bristol, Avon BS11 9YA\n",
            "1  Unit 27, Verey Road, Woodside Industrial Estat...\n",
            "2           Cradock Road, Reading, Berkshire RG2 0EE\n",
            "3                  Fazeley Street, Birmingham B5 5SE\n",
            "4             Adderley Road South, Birmingham B8 1AD\n",
            "5             Park Lane, Oldbury, Birmingham B69 4JX\n",
            "6                    Robeys Lane, Tamworth,  B78 1AR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGrQMKlgrVqO"
      },
      "source": [
        "## Storing more than one piece of information\n",
        "\n",
        "If you want to store more than just one piece of information there are different ways to do that, often depending on the nature of the page you are scraping. \n",
        "\n",
        "One is to identify the tag containing *all* the elements of info you want. In this case, although the *addresses* are inside `<h3>` tags, the entry for *all* of the info about each track is contained within `<div class=\"trackintro\">` like so:\n",
        "\n",
        "```{html}\n",
        "<div class=\"trackintro\">\n",
        "        <a href=\"http://www.uk-go-karting.com/tracks/birmingham/grand-prix-karting\"><img src=\"http://www.uk-go-karting.com/images/tracks/thumbs/3-1.jpg\" alt=\"Grand Prix Karting\"></a>\n",
        "        <h2><a href=\"http://www.uk-go-karting.com/tracks/birmingham/grand-prix-karting\">Grand Prix Karting</a></h2>\n",
        "        <h3>Adderley Road South, Birmingham B8 1AD</h3>\n",
        "        <p>Track length: 970m | <a href=\"http://www.uk-go-karting.com/tracks/birmingham/grand-prix-karting\">track details and activities</a> | <a onclick=\"_gaq.push(['_trackEvent(Karting Enquiry, County Page, Birmingham'])\" href=\"http://www.uk-go-karting.com/calculate?track_id=3\">book karting here</a></p>\n",
        "```\n",
        "\n",
        "So instead of targeting `<h3>` and looping through the matches we can *first* target that `<div class=\"trackintro\">`, loop through those, and *then* within the matches, extract the *first* (and only) `<h3>` and other items. \n",
        "\n",
        "Here's the code changed to do that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SncSxNLmDhZ1"
      },
      "source": [
        "#create a list of counties that we will need to generate URLs\n",
        "counties = [\"avon\",\"bedfordshire\",\"berkshire\",\"birmingham\"]\n",
        "#store the base URL we will add those to\n",
        "baseurl = \"http://www.uk-go-karting.com/tracks/\"\n",
        "\n",
        "#Create a dataframe to store the data we are about to scrape\n",
        "#It has one column called 'title'\n",
        "#We call this dataframe 'df'\n",
        "df = pd.DataFrame(columns=[\"location\"])\n",
        "\n",
        "#start looping through our list\n",
        "for county in counties:\n",
        "  fullurl = baseurl+county\n",
        "  print(fullurl)\n",
        "  #Scrape the html at that url\n",
        "  html = scraperwiki.scrape(fullurl)\n",
        "  # turn our HTML into an lxml object\n",
        "  root = lxml.html.fromstring(html) \n",
        "  #There are 100 recordings on the page\n",
        "  #The titles are all in <div class=\"title\"> and then <a \n",
        "  #This targets the contents of those html tags\n",
        "  tracks = root.cssselect('div.trackintro')\n",
        "  #the results are always a list so we have to loop through it using a 'for' loop\n",
        "  for i in tracks:\n",
        "    #each item in the list is called i as it loops\n",
        "    #we could store all the contents and store that, then split later\n",
        "    print(\"WHOLE THING\",i.text_content())\n",
        "    #but here we drill down deeper to grab any <a href> tags inside a <h2>\n",
        "    links = i.cssselect('h2 a')\n",
        "    #and any <h3> tags\n",
        "    h3s = i.cssselect('h3')\n",
        "    #and any <p> tags\n",
        "    ps = i.cssselect('p')\n",
        "    #knowing that there's only one - or at least we're only interested in the first\n",
        "    #we can then store just that one, rather than having to loop\n",
        "    firstlink = links[0]\n",
        "    #and extract the text\n",
        "    trackname = firstlink.text_content()\n",
        "    #the same for h3\n",
        "    firsth3 = h3s[0]\n",
        "    address = firsth3.text_content()\n",
        "    print(address)\n",
        "    #and for the p\n",
        "    firstp = ps[0]\n",
        "    length = firstp.text_content()\n",
        "    #Now we can store all in that variable called 'df' \n",
        "    df = df.append({\n",
        "      \"location\" : address,\n",
        "      \"track\" : trackname,\n",
        "      \"length\" : length\n",
        "      }, ignore_index=True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmAnt2iZG6il"
      },
      "source": [
        "There are other ways of achieving similar results. But this is the simplest strategy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuZj_TqwGB2f",
        "outputId": "ca43cf32-5f37-4500-d9e8-783b4724efaa"
      },
      "source": [
        "#Once the loop has finished we can take a look at the data\n",
        "print(df)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                            location  ...                                   track\n",
            "0              Avonmouth Way, Bristol, Avon BS11 9YA  ...                      Bristol Go Karting\n",
            "1  Unit 27, Verey Road, Woodside Industrial Estat...  ...                    Dunstable Go Karting\n",
            "2           Cradock Road, Reading, Berkshire RG2 0EE  ...                      Reading Go Karting\n",
            "3                  Fazeley Street, Birmingham B5 5SE  ...  Teamworks Karting Birmingham (Central)\n",
            "4             Adderley Road South, Birmingham B8 1AD  ...                      Grand Prix Karting\n",
            "5             Park Lane, Oldbury, Birmingham B69 4JX  ...                   Birmingham Go Karting\n",
            "6                    Robeys Lane, Tamworth,  B78 1AR  ...                Daytona Tamworth Karting\n",
            "\n",
            "[7 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh73Sbovc9To"
      },
      "source": [
        "## Exporting the data\n",
        "\n",
        "The `pandas` library has another function for exporting data: `to_csv()`.\n",
        "\n",
        "It needs to be attached to the name of the dataframe variable with a period, then, in the brackets, you specify the name of the file you want to export it as. Make sure this ends in '.csv' so it can be used in a spreadsheet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9FDGDX0__eA"
      },
      "source": [
        "#And we can export it\n",
        "df.to_csv(\"scrapeddata.csv\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCtq8ih5dNaE"
      },
      "source": [
        "## Downloading the data\n",
        "\n",
        "Once exported, it should appear in the file explorer in Google Colab on the left hand side. Click on the folder icon to open this up and you should see the file you just created (there's a refresh button above if you can't).\n",
        "\n",
        "Hover over the file name to see three dots, then click on those to select **Download** and download to your computer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOvob6z3hEmq"
      },
      "source": [
        ""
      ]
    }
  ]
}