{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"anExampleScraperList_multipleItems.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyN9kfJxyqNrvyra5GQLpSgH"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"fAT7xLYe_scU"},"source":["# An example scraper - this time grabbing more than one piece of info\n","\n","The code below can be copied and adapted to create your own scraper. This builds on [a previous scraper which introduced the use of lists in scraping](https://github.com/paulbradshaw/MED7369-Specialist-Investigative-Journalism/blob/master/python/anExampleScraperList.ipynb).\n","\n","The first part installs all the libraries..."]},{"cell_type":"code","metadata":{"id":"sE2vW-IX9kYX","executionInfo":{"status":"ok","timestamp":1647287701917,"user_tz":0,"elapsed":563,"user":{"displayName":"Paul Bradshaw","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYQSCAPAX7cikAjthcRciH5mrSmVPNLmT_aEplrw=s64","userId":"01457212023850926122"}}},"source":["#install the libraries \n","#requests is a library for fetching webpages from a URL\n","import requests\n","#BeautifulSoup is a library for scraping webpages\n","from bs4 import BeautifulSoup\n","#the pandas library which is used to work with data - we call it 'pd' here so we have to type less!\n","import pandas as pd"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sn8Zsx4fiC_u"},"source":["## The previous code\n","\n","Previously we looped through each item in a list and added it to a base url using the `+` operator, then scraped something from that URL.\n","\n","We also stored the results in a dataframe. \n","\n","Here's the code that we got to:"]},{"cell_type":"code","metadata":{"id":"9Yw0QKTJ_qpu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647287773628,"user_tz":0,"elapsed":4703,"user":{"displayName":"Paul Bradshaw","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYQSCAPAX7cikAjthcRciH5mrSmVPNLmT_aEplrw=s64","userId":"01457212023850926122"}},"outputId":"9a5438dc-142f-42a3-a931-b57a31db50a9"},"source":["#create a list of counties that we will need to generate URLs\n","counties = [\"avon\",\"bedfordshire\",\"berkshire\",\"birmingham\"]\n","#store the base URL we will add those to\n","baseurl = \"http://www.uk-go-karting.com/tracks/\"\n","\n","#create an empty list to store our addresses\n","addresslist = []\n","\n","#start looping through our list\n","for county in counties:\n","  fullurl = baseurl+county\n","  print(fullurl)\n","  #Scrape the html at that url\n","  html = requests.get(fullurl)\n","  # turn our HTML into a BeautifulSoup object\n","  soup = BeautifulSoup(html.content) \n","  #The names are all in <h3> - a change from our previous code\n","  #This targets the contents of those html tags\n","  addresses = soup.select('h3')\n","  #the results are always a list so we have to loop through it using a 'for' loop\n","  for i in addresses:\n","    #each item in the list is called i as it loops\n","    print(i)\n","    #on its own it includes tags, but we can attach .get_text() to translate it into text\n","    address = i.get_text()\n","    print(address)\n","    #add to the previously empty list\n","    addresslist.append(address)\n","\n","#Create a dataframe to store the data we scraped\n","#It has one column called 'location'\n","#We store the list 'addresslist' in that column\n","#We call this dataframe 'df'\n","df = pd.DataFrame({\"location\": addresslist})\n"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["http://www.uk-go-karting.com/tracks/avon\n","<h3>Avonmouth Way, Bristol, Avon BS11 9YA</h3>\n","Avonmouth Way, Bristol, Avon BS11 9YA\n","http://www.uk-go-karting.com/tracks/bedfordshire\n","<h3>Unit 27, Verey Road, Woodside Industrial Estate, Dunstable, Bedfordshire LU5 4TT</h3>\n","Unit 27, Verey Road, Woodside Industrial Estate, Dunstable, Bedfordshire LU5 4TT\n","http://www.uk-go-karting.com/tracks/berkshire\n","<h3>Cradock Road, Reading, Berkshire RG2 0EE</h3>\n","Cradock Road, Reading, Berkshire RG2 0EE\n","http://www.uk-go-karting.com/tracks/birmingham\n","<h3>Fazeley Street, Birmingham B5 5SE</h3>\n","Fazeley Street, Birmingham B5 5SE\n","<h3>Adderley Road South, Birmingham B8 1AD</h3>\n","Adderley Road South, Birmingham B8 1AD\n","<h3>Park Lane, Oldbury, Birmingham B69 4JX</h3>\n","Park Lane, Oldbury, Birmingham B69 4JX\n","<h3>Robeys Lane, Tamworth,  B78 1AR</h3>\n","Robeys Lane, Tamworth,  B78 1AR\n"]}]},{"cell_type":"code","metadata":{"id":"FIdecPU__881","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647287773883,"user_tz":0,"elapsed":4,"user":{"displayName":"Paul Bradshaw","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYQSCAPAX7cikAjthcRciH5mrSmVPNLmT_aEplrw=s64","userId":"01457212023850926122"}},"outputId":"ce3712bb-b3ec-4092-85cc-f21f9fd94989"},"source":["#Once the loop has finished we can take a look at the data\n","print(df)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["                                            location\n","0              Avonmouth Way, Bristol, Avon BS11 9YA\n","1  Unit 27, Verey Road, Woodside Industrial Estat...\n","2           Cradock Road, Reading, Berkshire RG2 0EE\n","3                  Fazeley Street, Birmingham B5 5SE\n","4             Adderley Road South, Birmingham B8 1AD\n","5             Park Lane, Oldbury, Birmingham B69 4JX\n","6                    Robeys Lane, Tamworth,  B78 1AR\n"]}]},{"cell_type":"markdown","metadata":{"id":"LGrQMKlgrVqO"},"source":["## Storing more than one piece of information\n","\n","If you want to store more than just one piece of information there are different ways to do that, often depending on the nature of the page you are scraping. \n","\n","One is to identify the tag containing *all* the elements of info you want. In this case, although the *addresses* are inside `<h3>` tags, the entry for *all* of the info about each track is contained within `<div class=\"trackintro\">` like so:\n","\n","```{html}\n","<div class=\"trackintro\">\n","        <a href=\"http://www.uk-go-karting.com/tracks/birmingham/grand-prix-karting\"><img src=\"http://www.uk-go-karting.com/images/tracks/thumbs/3-1.jpg\" alt=\"Grand Prix Karting\"></a>\n","        <h2><a href=\"http://www.uk-go-karting.com/tracks/birmingham/grand-prix-karting\">Grand Prix Karting</a></h2>\n","        <h3>Adderley Road South, Birmingham B8 1AD</h3>\n","        <p>Track length: 970m | <a href=\"http://www.uk-go-karting.com/tracks/birmingham/grand-prix-karting\">track details and activities</a> | <a onclick=\"_gaq.push(['_trackEvent(Karting Enquiry, County Page, Birmingham'])\" href=\"http://www.uk-go-karting.com/calculate?track_id=3\">book karting here</a></p>\n","```\n","\n","So instead of targeting `<h3>` and looping through the matches we can *first* target that `<div class=\"trackintro\">`, loop through those, and *then* within the matches, extract the *first* (and only) `<h3>` and other items. \n","\n","Here's the code changed to do that:"]},{"cell_type":"code","metadata":{"id":"SncSxNLmDhZ1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"38ac64a4-fbd6-4977-a317-d2f6e12c71cd"},"source":["#create a list of counties that we will need to generate URLs\n","counties = [\"avon\",\"bedfordshire\",\"berkshire\",\"birmingham\"]\n","#store the base URL we will add those to\n","baseurl = \"http://www.uk-go-karting.com/tracks/\"\n","\n","#create an empty dataframe to store the data\n","df = pd.DataFrame()\n","\n","#start looping through our list\n","for county in counties:\n","  fullurl = baseurl+county\n","  print(fullurl)\n","  #Scrape the html at that url\n","  html = requests.get(fullurl)\n","  # turn our HTML into a BeautifulSoup object\n","  soup = BeautifulSoup(html.content) \n","  #There are 100 recordings on the page\n","  #The titles are all in <div class=\"title\"> and then <a \n","  #This targets the contents of those html tags\n","  tracks = soup.select('div.trackintro')\n","  #the results are always a list so we have to loop through it using a 'for' loop\n","  for i in tracks:\n","    #each item in the list is called i as it loops\n","    #we could store all the contents and store that, then split later\n","    #print(\"WHOLE THING\",i.text_content())\n","    #grab the image location\n","    imgs = i.select('img')\n","    firstimg = imgs[0]\n","    imgsrc = firstimg['src']\n","    print(imgsrc)\n","    #but here we drill down deeper to grab any <a href> tags inside a <h2>\n","    links = i.select('h2 a')\n","    print(links[0]['href'])\n","    #and any <h3> tags\n","    h3s = i.select('h3')\n","    #and any <p> tags\n","    ps = i.select('p')\n","    #knowing that there's only one - or at least we're only interested in the first\n","    #we can then store just that one, rather than having to loop\n","    firstlink = links[0]\n","    #and extract the text\n","    trackname = firstlink.get_text()\n","    #the same for h3\n","    firsth3 = h3s[0]\n","    address = firsth3.get_text()\n","    print(address)\n","    #and for the p\n","    firstp = ps[0]\n","    length = firstp.get_text()\n","    #Now we can store all in that variable called 'df' \n","    df = df.append({\n","      \"location\" : address,\n","      \"track\" : trackname,\n","      \"length\" : length\n","      }, ignore_index=True)\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["http://www.uk-go-karting.com/tracks/avon\n","http://www.uk-go-karting.com/images/tracks/thumbs/24-1.jpg\n","http://www.uk-go-karting.com/tracks/avon/the-raceway\n","Avonmouth Way, Bristol, Avon BS11 9YA\n","http://www.uk-go-karting.com/tracks/bedfordshire\n","http://www.uk-go-karting.com/images/tracks/thumbs/372-1.jpg\n","http://www.uk-go-karting.com/tracks/bedfordshire/dunstable-go-karting\n","Unit 27, Verey Road, Woodside Industrial Estate, Dunstable, Bedfordshire LU5 4TT\n","http://www.uk-go-karting.com/tracks/berkshire\n","http://www.uk-go-karting.com/images/tracks/thumbs/345-1.jpg\n","http://www.uk-go-karting.com/tracks/berkshire/reading-go-karting\n","Cradock Road, Reading, Berkshire RG2 0EE\n","http://www.uk-go-karting.com/tracks/birmingham\n"]}]},{"cell_type":"markdown","metadata":{"id":"PmAnt2iZG6il"},"source":["There are other ways of achieving similar results. But this is the simplest strategy."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uuZj_TqwGB2f","executionInfo":{"status":"ok","timestamp":1615305272999,"user_tz":0,"elapsed":1324,"user":{"displayName":"Paul Bradshaw","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgYQSCAPAX7cikAjthcRciH5mrSmVPNLmT_aEplrw=s64","userId":"01457212023850926122"}},"outputId":"b7401bd4-a887-4b85-abbc-70b0c243a2a9"},"source":["#Once the loop has finished we can take a look at the data\n","print(df)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["                                            location  ...                                             length\n","0              Avonmouth Way, Bristol, Avon BS11 9YA  ...  Track length: 500m | track details and activit...\n","1  Unit 27, Verey Road, Woodside Industrial Estat...  ...  Track length: 500m | track details and activit...\n","2           Cradock Road, Reading, Berkshire RG2 0EE  ...  Track length: 450m | track details and activit...\n","3                  Fazeley Street, Birmingham B5 5SE  ...  Track length: 450m | track details and activit...\n","4             Adderley Road South, Birmingham B8 1AD  ...  Track length: 970m | track details and activit...\n","5             Park Lane, Oldbury, Birmingham B69 4JX  ...  Track length: 1000m | track details and activi...\n","6                    Robeys Lane, Tamworth,  B78 1AR  ...  Track length: 1000m | track details and activi...\n","\n","[7 rows x 3 columns]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Bh73Sbovc9To"},"source":["## Exporting the data\n","\n","The `pandas` library has another function for exporting data: `to_csv()`.\n","\n","It needs to be attached to the name of the dataframe variable with a period, then, in the brackets, you specify the name of the file you want to export it as. Make sure this ends in '.csv' so it can be used in a spreadsheet."]},{"cell_type":"code","metadata":{"id":"i9FDGDX0__eA"},"source":["#And we can export it\n","df.to_csv(\"scrapeddata.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wCtq8ih5dNaE"},"source":["## Downloading the data\n","\n","Once exported, it should appear in the file explorer in Google Colab on the left hand side. Click on the folder icon to open this up and you should see the file you just created (there's a refresh button above if you can't).\n","\n","Hover over the file name to see three dots, then click on those to select **Download** and download to your computer."]},{"cell_type":"markdown","metadata":{"id":"BOvob6z3hEmq"},"source":[""]}]}